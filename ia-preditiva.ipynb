{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inteligência Artificial Preditiva: Auto ML\n",
    "\n",
    "## Este notebook tem como objetivo explorar 5 principais modelos de inteligência artificial preditiva:\n",
    "### Regressão \n",
    "    1. Regressão Linear\n",
    "    2. K-Nearest Neighbors Regressor\n",
    "### Classificação\n",
    "    3. Naive Bayes\n",
    "    4. Decision Tree: Gini\n",
    "    5. Decision Tree: Entropia\n",
    "    6. K-Nearest Neighbors (KNN)\n",
    "    7. XGBoost\n",
    "\n",
    "## Desses 7, usaremos a que melhor performou para a base de dados que você irá inserir!\n",
    "\n",
    "#### CODEOWNERS:\n",
    "- **Rafael Cruz** [GitHub](https://github.com/RafaelBarretoCruz)\n",
    "- **Gabriel Martins** [GitHub](https://github.com/GabrielOliveiraGerminare1H)\n",
    "- **Fellipe Meira** [GitHub](https://github.com/fellipemeiraGerminare)\n",
    "\n"
   ],
   "id": "44c70cb17dd9cfa8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Importando bibliotecas </h1>",
   "id": "71f933bb3f5861dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importações das bibliotecas gerais\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Importações dos modelos e separação de treino/teste\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Importações das métricas e pré-processamento\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    median_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "        accuracy_score,\n",
    "        confusion_matrix,\n",
    "        f1_score,\n",
    "        ConfusionMatrixDisplay, \\\n",
    "        precision_score,\n",
    "        recall_score)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Importações para visualizações\n",
    "import yellowbrick.classifier as yb\n",
    "\n",
    "\n",
    "# Setando opções do pandas para não diminuir o número de colunas exibidas\n",
    "pd.set_option('display.max_columns', None)\n"
   ],
   "id": "997df5bc6debb463",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Carregando a base de dados </h1>",
   "id": "bafa80865e001be3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Solicita o nome do arquivo CSV ao usuário\n",
    "# csv_nome = input(\"Digite o nome do arquivo CSV (incluindo a extensão .csv): \")\n",
    "\n",
    "csv_nome=\"vinho-reduzido.csv\" # Exemplo fixo para testes\n",
    "\n",
    "caminho = os.path.join(os.getcwd(), \"datasets\", csv_nome)\n",
    "\n",
    "# Detecta o delimitador automaticamente\n",
    "with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
    "    sniffer = csv.Sniffer()\n",
    "    sample = f.read(2048)\n",
    "    f.seek(0)\n",
    "    delimiter_encontrado = sniffer.sniff(sample).delimiter\n",
    "\n",
    "print(f\"Delimitador detectado: '{delimiter_encontrado}'\")\n",
    "\n",
    "# Carrega o arquivo CSV usando o delimitador detectado\n",
    "try:\n",
    "    data = pd.read_csv(caminho, delimiter=delimiter_encontrado)\n",
    "    print(\"Arquivo carregado!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo não encontrado. Verifique o nome e o caminho do arquivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao carregar o arquivo: {e}\")"
   ],
   "id": "ddee5a5713eeed2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Primeira visualização da base</h1>",
   "id": "637b4fd31fbf2555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(data.head())",
   "id": "f2f5c9ccac85add6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Definindo a coluna que será prevista (variável target)</h1>",
   "id": "f62666cee5796e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pedindo ao usuário para definir a coluna target\n",
    "# coluna_target = input(\"Digite o nome da coluna que você deseja prever (variável target): \")\n",
    "\n",
    "coluna_target='type'  # Exemplo fixo para testes\n",
    "\n",
    "# Verificando se a coluna existe na base de dados\n",
    "if coluna_target not in data.columns:\n",
    "    raise ValueError(f\"A coluna '{coluna_target}' não existe na base de dados.\")\n",
    "else:\n",
    "    x = data.drop(columns=[coluna_target])\n",
    "    y = data[coluna_target]"
   ],
   "id": "97c457fd0ebb1575",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Pré-processamento dos dados </h1>",
   "id": "7a2430a4f064993e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classes_por_extenso = y.unique()\n",
    "\n",
    "\n",
    "# Identificar colunas numéricas e categóricas\n",
    "num_cols = x.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = x.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Codificar colunas categóricas (LabelEncoder não precisa de Grid Search)\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    x[col] = le.fit_transform(x[col].astype(str))\n",
    "\n",
    "# Definir pipelines para pré-processamento (imputação e escalonamento)\n",
    "# Para imputação, vamos considerar 'mean', 'median' e 'most_frequent'\n",
    "# Para escalonamento, vamos considerar StandardScaler e MinMaxScaler\n",
    "\n",
    "# Apenas se houver colunas numéricas\n",
    "if len(num_cols) > 0:\n",
    "    pipeline_preprocessing = Pipeline([\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    param_grid_preprocessing = {\n",
    "        'imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'scaler': [StandardScaler(), MinMaxScaler()]\n",
    "    }\n",
    "\n",
    "    grid_search_preprocessing = GridSearchCV(pipeline_preprocessing, param_grid_preprocessing, cv=5, scoring='r2' if y.dtype != 'object' else 'accuracy')\n",
    "    grid_search_preprocessing.fit(x[num_cols], y)\n",
    "\n",
    "    print(\"Melhores parâmetros para pré-processamento:\", grid_search_preprocessing.best_params_)\n",
    "\n",
    "    # Aplicar o melhor pré-processamento aos dados numéricos\n",
    "    best_imputer = grid_search_preprocessing.best_estimator_.named_steps['imputer']\n",
    "    best_scaler = grid_search_preprocessing.best_estimator_.named_steps['scaler']\n",
    "\n",
    "    x[num_cols] = best_imputer.fit_transform(x[num_cols])\n",
    "    x[num_cols] = best_scaler.fit_transform(x[num_cols])\n",
    "else:\n",
    "    print(\"Não há colunas numéricas para pré-processamento de imputação e escalonamento.\")\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    le_y = LabelEncoder()\n",
    "    y = le_y.fit_transform(y)"
   ],
   "id": "accf766caa8f258e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Visualizando novamente a base de dados </h1>",
   "id": "cbb8e1499f1a9266"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(x.head())",
   "id": "9858ca6721358bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Separação entre treino e teste </h1>",
   "id": "96f3798782f42e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n"
   ],
   "id": "8faa21c896148b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Detectando o tipo da variável target para seguirmos com o modelo</h1>",
   "id": "aa8fabee929b8fc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resultados = {}\n",
    "\n",
    "# Determinar se é um problema de classificação ou regressão\n",
    "if len(np.unique(y)) < 20 or y.dtype == 'object':\n",
    "    problema = \"classificacao\"\n",
    "    modelos = {\n",
    "        \"Decision Tree Gini\": DecisionTreeClassifier(criterion='gini', random_state=42),\n",
    "        \"Decision Tree Entropia\": DecisionTreeClassifier(criterion='entropy', random_state=42),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42, verbosity=0)\n",
    "    }\n",
    "    param_grids = {\n",
    "        \"Decision Tree Gini\": {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'ccp_alpha': [0.0, 0.01, 0.1],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        },\n",
    "        \"Decision Tree Entropia\": {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'ccp_alpha': [0.0, 0.01, 0.1],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        },\n",
    "        \"Naive Bayes\": {\n",
    "            'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski', 'cosine'],\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [3, 6],\n",
    "            \"learning_rate\": [0.01, 0.1],\n",
    "            \"subsample\": [0.8],\n",
    "            \"colsample_bytree\": [0.8,  1.0],\n",
    "            \"random_state\": [42],\n",
    "            \"tree_method\": [\"exact\", \"approx\", \"gpu_hist\"]\n",
    "        }\n",
    "    }\n",
    "    scoring_metric = 'accuracy'\n",
    "# Se for regressão, apenas Regressão Linear\n",
    "else:\n",
    "    problema = \"regressao\"\n",
    "    modelos = {\n",
    "        \"Regressão Linear\": LinearRegression(),\n",
    "        \"KNN Regressor\": KNeighborsRegressor()\n",
    "    }\n",
    "    param_grids = {\n",
    "        \"Regressão Linear\": {\n",
    "            \"fit_intercept\": [True, False],\n",
    "            \"positive\": [True, False]\n",
    "        },\n",
    "        \"KNN Regressor\": {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski', 'cosine'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'leaf_size': [20, 30, 40],\n",
    "        }\n",
    "    }\n",
    "    scoring_metric = 'r2'"
   ],
   "id": "4c0c55fe2bf45bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Treinando e avaliando os modelos </h1>",
   "id": "c82b9f7eeb6e6962"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_models = {}\n",
    "nome_base = csv_nome.split(\".\")[0]\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    os.makedirs(\"pickles\", exist_ok=True)\n",
    "    pickle_path = os.path.join(\"pickles\", f\"modelo_{nome}_{nome_base}.pkl\")\n",
    "\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            best_models[nome] = pickle.load(f)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(modelo, param_grids[nome], cv=5, scoring=scoring_metric)\n",
    "        grid_search.fit(x_treino, y_treino)\n",
    "\n",
    "        best_models[nome] = grid_search.best_estimator_\n",
    "\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(best_models[nome], f)\n",
    "\n",
    "    y_pred = best_models[nome].predict(x_teste)\n",
    "\n",
    "    if problema == \"classificacao\":\n",
    "        acc = accuracy_score(y_teste, y_pred)\n",
    "        f1 = f1_score(y_teste, y_pred, average=\"weighted\")\n",
    "        precision = precision_score(y_teste, y_pred, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(y_teste, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        resultados[nome] = {\n",
    "            \"Acurácia\": acc,\n",
    "            \"F1-Score\": f1,\n",
    "            \"Precisão\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"Melhores Parâmetros\": grid_search.best_params_\n",
    "        }\n",
    "    else:\n",
    "        mse = mean_squared_error(y_teste, y_pred)\n",
    "        r2 = r2_score(y_teste, y_pred)\n",
    "        mae = median_absolute_error(y_teste, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_teste, y_pred)\n",
    "\n",
    "        resultados[nome] = {\n",
    "            \"MSE\": mse,\n",
    "            \"R²\": r2,\n",
    "            \"MAE\": mae,\n",
    "            \"MAPE\": mape,\n",
    "            \"Melhores Parâmetros\": grid_search.best_params_\n",
    "        }"
   ],
   "id": "bae8785dc1c92cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Resultados dos modelos </h1>",
   "id": "3a4b404de1259fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(pd.DataFrame(resultados))\n",
    "\n",
    "# Transpondo para melhor visualização\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "display(df_resultados)\n",
    "\n",
    "# Identificar melhor modelo de classificação\n",
    "if problema == \"classificacao\":\n",
    "    melhor = df_resultados[\"Acurácia\"].idxmax()\n",
    "\n",
    "# Aplicando para regressão\n",
    "else:\n",
    "    melhor = df_resultados[\"R²\"].idxmax()\n"
   ],
   "id": "1232cc79999e45df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Modelo campeão </h1>",
   "id": "334b6fe7ffdeb8fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"O melhor modelo é: {melhor}\")\n",
    "y_pred = best_models[melhor].predict(x_teste)"
   ],
   "id": "25cb972d800a2782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Mais visualizações do modelo campeão</h1>",
   "id": "a985298d1a39f3db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if problema == \"classificacao\":\n",
    "    plt.figure(figsize=(8,5))\n",
    "    ax = sns.barplot(x=df_resultados.index, y='Acurácia', data=df_resultados, palette=\"viridis\")\n",
    "    plt.ylim(0, 1.2)\n",
    "    plt.title(\"Comparação de Acurácia dos Modelos\")\n",
    "    plt.ylabel(\"Acurácia\")\n",
    "    plt.xlabel(\"Modelos\")\n",
    "    plt.xticks(rotation=45)\n",
    "    # Para cada barra, adicionar o valor percentual\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, labels=[f'{v.get_height()*100:.2f}%' for v in container])\n",
    "    plt.show()\n",
    "\n",
    "    matriz = confusion_matrix(y_teste, y_pred, labels=best_models[melhor].classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matriz, display_labels=classes_por_extenso)\n",
    "    disp.plot(cmap=plt.cm.viridis)\n",
    "    plt.grid(False)\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.show()\n",
    "\n",
    "    visualizer = yb.ClassificationReport(\n",
    "                                        best_models[melhor],\n",
    "                                        classes=classes_por_extenso,\n",
    "                                        cmap= \"Greens\",\n",
    "                                        support=True)\n",
    "    visualizer.fit(x_treino, y_treino)\n",
    "    visualizer.score(x_teste, y_teste)\n",
    "    visualizer.show()\n",
    "\n",
    "else:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    ax = sns.barplot(x=df_resultados.index, y='R²', data=df_resultados, palette=\"viridis\")\n",
    "    plt.ylim(0, 1.2)\n",
    "    plt.title(\"Comparação de R² dos Modelos\")\n",
    "    plt.ylabel(\"R²\")\n",
    "    plt.xlabel(\"Modelos\")\n",
    "    plt.xticks(rotation=45)\n",
    "    # Para cada barra, adicionar o valor percentual\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, labels=[f'{v.get_height()*100:.2f}%' for v in container])\n",
    "    plt.show()\n",
    "\n",
    "    # Cálculo dos resíduos\n",
    "    residuos = y_teste - y_pred\n",
    "    media_resid = np.mean(residuos)\n",
    "    std_resid = np.std(residuos)\n",
    "\n",
    "    # Dispersão resíduos vs preditos\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(y_pred, residuos, alpha=0.4, s=15, c=residuos, cmap=\"viridis\")\n",
    "    plt.axhline(y=0, color=\"r\", linestyle=\"--\", label=\"Média (0)\")\n",
    "    plt.axhline(media_resid + 2*std_resid, color=\"orange\", linestyle=\"--\", label=\"+2σ\")\n",
    "    plt.axhline(media_resid - 2*std_resid, color=\"orange\", linestyle=\"--\", label=\"-2σ\")\n",
    "    plt.xlabel(\"Valor Predito\")\n",
    "    plt.ylabel(\"Resíduo (Real - Predito)\")\n",
    "    plt.title(\"Resíduos do Modelo de Regressão\")\n",
    "    plt.legend()\n",
    "    plt.colorbar(label=\"Resíduo\")\n",
    "    limite = max(abs(residuos.min()), abs(residuos.max()))\n",
    "    plt.ylim(-limite, limite)\n",
    "    plt.show()\n",
    "\n",
    "    # Histograma da distribuição dos resíduos\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(residuos, bins=30, kde=True, color=\"darkcyan\", alpha=0.6)\n",
    "    plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"0\")\n",
    "    plt.axvline(media_resid, color=\"black\", linestyle=\"--\", label=f\"Média ({media_resid:.2f})\")\n",
    "    plt.title(\"Distribuição dos Resíduos\")\n",
    "    plt.xlabel(\"Resíduo\")\n",
    "    plt.ylabel(\"Frequência\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "id": "5ea70334679a56ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
